Competitions: You can optionally upload an unlabeled test set. There is a task type 'machine learning challenge' for this. Then you will get a Kaggle-like competition. Competitors will have to upload the predictions for the unlabeled data. 

That said, OpenML is primarily a collaboration platform, not a competition platform (Kaggle already does this quite well). OpenML is meant to bring the scientific research experience online. It's default mode is to give all the data, and it fights cheating with reproducibility and reputation. The code is open and people can rerun and study each other's code. You CAN cheat, but it is risky because others will find out and that would be quite humiliating. We may soon reflect this by an online reputation score like on stackoverflow, where 'cheaters' can be downvoted and will quickly get a bad name. Your reputation may also affect which data you see, i.e. people may choose to only share data with other people if they have a high enough reputation (e.g. if they also share data or help others in other ways). Or they may choose to setup a 'circle' of researchers and only allow trustworthy people in.

Linking to publications: we REALLY want to do this, we just need more time and people. Simply things like adding a field for an ArXiv url can be done quickly if someone simply takes the lead on that. You could also be more ambitious and e.g. add a link to ResearchGate or Google Scholar so that people's publications can be pulled in from there. We recently submitted a proposal to do text mining on the scientific literature so that we can mine papers for the datasets and algorithms used in them, so we can link those papers to the datasets and algorithms in OpenML.
